{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take a subsample of 600000 from the data to work with, since there are more than 1.2 million rows\n",
    "\n",
    "total_rows = 1264216\n",
    "sample_size = 600000\n",
    "skiprows = sorted(random.sample(range(total_rows), total_rows-sample_size))\n",
    "\n",
    "dtype = {'Id': pd.Int64Dtype(), 'OwnerUserId': pd.Int64Dtype(), 'CreationDate': np.datetime64, 'ClosedDate': np.datetime64, 'Title': object, 'Body': object}\n",
    "data = pd.read_csv('Questions.csv', engine='python', dtype=dtype, skiprows=skiprows)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the respective tags for the selected subsample of the questions\n",
    "\n",
    "tags = pd.read_csv('Tags.csv')\n",
    "tags = tags[tags['Id'].isin(data['Id'])]\n",
    "tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#select features to use\n",
    "\n",
    "X = data[['Id', 'Title', 'Body']]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip html tags from question body using beautifulsoup\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    X.loc[X.index[i], 'Body'] = BeautifulSoup(X.loc[X.index[i], 'Body']).get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to lower case\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    X.loc[X.index[i], 'Title'] = X.loc[X.index[i], 'Title'].lower()\n",
    "    X.loc[X.index[i], 'Body'] = X.loc[X.index[i], 'Body'].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove numbers\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    X.loc[X.index[i], 'Title'] = re.sub(r'\\d+', '', X.loc[X.index[i], 'Title'])\n",
    "    X.loc[X.index[i], 'Body'] = re.sub(r'\\d+', '', X.loc[X.index[i], 'Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove punctuation\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    X.loc[X.index[i], 'Title'] = X.loc[X.index[i], 'Title'].translate(X.loc[X.index[i], 'Title'].maketrans('', '', string.punctuation))\n",
    "    X.loc[X.index[i], 'Body'] = X.loc[X.index[i], 'Body'].translate(X.loc[X.index[i], 'Body'].maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove whitespaces\n",
    "\n",
    "for i in range(X.shape[0]):\n",
    "    X.loc[X.index[i], 'Title'] = X.loc[X.index[i], 'Title'].strip()\n",
    "    X.loc[X.index[i], 'Body'] = X.loc[X.index[i], 'Body'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "for i in range(X.shape[0]):\n",
    "    X.loc[X.index[i], 'Title'] = \" \".join([word for word in nltk.tokenize.word_tokenize(X.loc[X.index[i], 'Title']) if not word in stopwords])\n",
    "    X.loc[X.index[i], 'Body'] = \" \".join([word for word in nltk.tokenize.word_tokenize(X.loc[X.index[i], 'Body']) if not word in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform stemming\n",
    "\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "for i in range(X.shape[0]):\n",
    "    X.loc[X.index[i], 'Title'] = \" \".join([stemmer.stem(word) for word in nltk.tokenize.word_tokenize(X.loc[X.index[i], 'Title'])])\n",
    "    X.loc[X.index[i], 'Body'] = \" \".join([stemmer.stem(word) for word in nltk.tokenize.word_tokenize(X.loc[X.index[i], 'Body'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframes as feathers\n",
    "\n",
    "data.to_feather('./data.ftr')\n",
    "X.to_feather('./X.ftr')\n",
    "\n",
    "#sliced dataframes need to be reindexed or converted to csv before saving as feathers\n",
    "\n",
    "tags.to_csv('./tags.csv', index=False)\n",
    "tags = pd.read_csv('./tags.csv')\n",
    "tags.to_feather('./tags.ftr')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
